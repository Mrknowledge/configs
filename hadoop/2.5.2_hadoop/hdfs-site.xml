<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>

	<property>
		<name>dfs.hosts</name>
		<value>/home/hdfs/hadoop-2.5.2/etc/hadoop/slaves</value>
		<description>Names a file that contains a list of hosts that are
		permitted to connect to the namenode. The full pathname of the file
		must be specified.  If the value is empty, all hosts are
		permitted.</description>
	</property>
	<property>
		<name>dfs.hosts.exclude</name>
		<value>/home/hdfs/hadoop-2.5.2/etc/hadoop/hosts.deny</value>
		<description>Names a file that contains a list of hosts that are
		not permitted to connect to the namenode.  The full pathname of the
		file must be specified.  If the value is empty, no hosts are
		excluded.</description>
	</property> 

	<property>
		<name>dfs.nameservices</name>
		<!--value>ns1,ns2</value-->
		<value>ns1</value>
		<description>
		Comma-separated list of nameservices.
		</description>
	</property>

	<property>
		<name>dfs.ha.namenodes.ns1</name>
		<value>nn1,nn2</value>
		<description>Unique identifiers for each NameNode in thenameservice</description>
	</property>
	<!--property>
		<name>dfs.ha.namenodes.ns2</name>
		<value>nn3,nn4</value>
		<description>Unique identifiers for each NameNode in thenameservice</description>
	</property-->

	<property>
		<name>dfs.namenode.rpc-address.ns1.nn1</name>
		<value>M1:8020</value>
	</property>
	<property>
		<name>dfs.namenode.http-address.ns1.nn1</name>
		<value>M1:50070</value>
	</property>
	<property>
		<name>dfs.namenode.servicerpc-address.ns1.nn1</name>
		<value>M1:53310</value>
	</property>
	<!--property>
		<name>dfs.namenode.secondary.http-address.ns1.nn1</name>
		<value>M1:50090</value>
	</property-->
	<property>
		<name>dfs.namenode.shared.edits.dir.ns1.nn1</name>
		<value>qjournal://D234:8485;D235:8485;D236:8485/ns1</value>
	</property>

	<property>
		<name>dfs.namenode.rpc-address.ns1.nn2</name>
		<value>M2:8020</value>
	</property>
	<property>
		<name>dfs.namenode.http-address.ns1.nn2</name>
		<value>M2:50070</value>
	</property>
	<property>
		<name>dfs.namenode.servicerpc-address.ns1.nn2</name>
		<value>M2:53310</value>
	</property>
	<!--property>
		<name>dfs.namenode.secondary.http-address.ns1.nn2</name>
		<value>M2:50090</value>
	</property-->
	<property>
		<name>dfs.client.failover.proxy.provider.ns1</name>
		<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
	</property>
	<property>
		<name>dfs.namenode.shared.edits.dir.ns1.nn2</name>
		<value>qjournal://D234:8485;D235:8485;D236:8485/ns1</value>
	</property>

	<!--property>
		<name>dfs.namenode.rpc-address.ns2.nn3</name>
		<value>M3:8020</value>
	</property>
	<property>
		<name>dfs.namenode.http-address.nn2.nn3</name>
		<value>M3:50070</value>
	</property>
	<property>
		<name>dfs.namenode.servicerpc-address.ns2.nn3</name>
		<value>M3:53310</value>
	</property>
	<property>
		<name>dfs.namenode.shared.edits.dir.ns2.nn3</name>
		<value>qjournal://D234:8485;D235:8485;D236:8485/ns2</value>
	</property>

	<property>
		<name>dfs.namenode.rpc-address.ns2.nn4</name>
		<value>M4:8020</value>
	</property>
	<property>
		<name>dfs.namenode.http-address.nn2.nn4</name>
		<value>M4:50070</value>
	</property>
	<property>
		<name>dfs.namenode.servicerpc-address.ns2.nn4</name>
		<value>M4:53310</value>
	</property>
	<property>
		<name>dfs.client.failover.proxy.provider.ns2</name>
		<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
	</property>
	<property>
		<name>dfs.namenode.shared.edits.dir.ns2.nn4</name>
		<value>qjournal://D234:8485;D235:8485;D236:8485/ns2</value>
	</property-->

	<property>
		<name>dfs.journalnode.edits.dir</name>
		<value>/home/hadoop/data0/journaldata</value>
	</property>
	<property>
		<name>dfs.ha.automatic-failover.enabled</name>
		<value>false</value>
	</property>

	<property>
		<name>dfs.namenode.checkpoint.txns</name>
		<value>100000</value>
	</property>
	<property>
		<name>dfs.namenode.num.extra.edits.retained</name>
		<value>100000</value>
	</property>
	<property>
		<name>dfs.namenode.max.extra.edits.segments.retained</name>
		<value>1000</value>
	</property>

	<property>
		<name>dfs.support.append</name>
		<value>true</value>
		<description>
		Does HDFS allow appends to files?
		</description>
	</property>

	<property>
		<name>dfs.namenode.name.dir</name>
		<value>file:/home/hadoop/data1/dfs/name,file:/home/hadoop/data2/dfs/name</value>
		<description>Determineswhere on the local filesystem the DFS name node should store the name table.</description>
		<final>true</final>
	</property>
	<property>
		<name>dfs.datanode.data.dir</name>
		<value>file:/home/hadoop/data1/dfs/data,file:/home/hadoop/data2/dfs/data,file:/home/hadoop/data3/dfs/data,file:/home/hadoop/data4/dfs/data</value>
		<description>Determines where on the local filesystem an DFS data node
		should store its blocks.  If this is a comma-delimited
		list of directories, then data will be stored in all named
		directories, typically on different devices.
		Directories that do not exist are ignored.
		</description>
		<final>true</final>
	</property>

	<property>
		<name>dfs.replication</name>
		<value>3</value>
	</property>
	<property>
		<name>dfs.datanode.failed.volumes.tolerated</name>
		<value>2</value>
		<description>The number of volumes that are allowed to
		fail before a datanode stops offering service. By default
		any volume failure will cause a datanode to shutdown.
		</description>
	</property>

	<property>
		<name>dfs.permissions</name>
		<value>false</value>
	</property>

	<property>
		<name>dfs.datanode.max.transfer.threads</name>
		<value>30720</value>
		<description>
			Specifies the maximum number of threads to use for transferring data
			in and out of the DN.
		</description>
	</property>
	<property>
		<name>dfs.datanode.handler.count</name>
		<value>8</value>
		<description>The number of server threads for the datanode.</description>
	</property>
	<property>
		<name>dfs.namenode.handler.count</name>
		<value>64</value>
		<description>The number of server threads for the namenode.</description>
	</property>

	<property>
		<name>dfs.client.block.write.replace-datanode-on-failure.enable</name>
		<value>true</value>
	</property>
	<property>
		<name>dfs.client.block.write.replace-datanode-on-failure.policy</name>
		<value>ALWAYS</value>
	</property>

	<property>
		<name>dfs.datanode.address</name>
		<value>${local.bind.address}:50010</value>
		<description>
		The datanode server address and port for data transfer.
		</description>
	</property>
	<property>
		<name>dfs.datanode.http.address</name>
		<value>${local.bind.address}:50075</value>
		<description>
		The datanode http server address and port.
		</description>
	</property>
	<property>
		<name>dfs.datanode.ipc.address</name>
		<value>${local.bind.address}:50020</value>
		<description>
		The datanode ipc server address and port.
		</description>
	</property>
	<property>
		<name>dfs.datanode.https.address</name>
		<value>${local.bind.address}:50475</value>
		<description>The datanode secure http server address and port.</description>
	</property>
	<property>
		<name>dfs.namenode.https-address</name>
		<value>${local.bind.address}:50470</value>
		<description>The namenode secure http server address and port.</description>
	</property>

	<property>
		<name>dfs.journalnode.rpc-address</name>
		<value>${local.bind.address}:8485</value>
		<description>
		The JournalNode RPC server address and port.
		</description>
	</property>
	<property>
		<name>dfs.journalnode.http-address</name>
		<value>${local.bind.address}:8480</value>
		<description>
		The address and port the JournalNode web UI listens on.
		If the port is 0 then the server will start on a free port.
		</description>
	</property>

<!-- i/o properties -->
	<property>
		<name>dfs.blocksize</name>
		<value>125829120</value>
		<description>
		  The default block size for new files, in bytes.
		  You can use the following suffix (case insensitive):
		  k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.),
		  Or provide complete size in bytes (such as 134217728 for 128 MB).
		</description>
	</property>
	<property>
		<name>dfs.stream-buffer-size</name>
		<value>131072</value>
		<description>The size of buffer to stream files.
		The size of this buffer should probably be a multiple of hardware
		page size (4096 on Intel x86), and it determines how much data is
		buffered during read and write operations.</description>
	</property>
	<property>
		<name>dfs.bytes-per-checksum</name>
		<value>16384</value>
		<description>The number of bytes per checksum.  Must not be larger than
		dfs.stream-buffer-size</description>
	</property>
	<property>
		<name>dfs.client-write-packet-size</name>
		<!--value>4194304</value-->
		<value>131072</value>
		<description>Packet size for clients to write</description>
	</property>

	<property>
		<name>dfs.client.socket-timeout</name>
		<value>480000</value>
	</property>
	<property>
		<name>dfs.socket.timeout</name>
		<value>480000</value>
	</property>
	<property>
		<name>dfs.datanode.socket.write.timeout</name>
		<value>480000</value>
	</property>

        <property>
                <name>ipc.maximum.data.length</name>
                <value>134217728</value>
        </property>

</configuration>
